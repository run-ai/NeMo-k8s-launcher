apiVersion: v1
kind: ConfigMap
metadata:
  name: nemo-gpt3-5b
data:
  gpt3_5b_hydra.yaml: |
    run:
      name: gpt3_5b
      results_dir: /workspace/train
      time_limit: '10:00:00'
      dependency: singleton
    trainer:
      num_nodes: 8
      devices: 1
      accelerator: gpu
      precision: bf16
      logger: false
      enable_checkpointing: false
      replace_sampler_ddp: false
      max_epochs: null
      max_steps: 200
      max_time: 05:23:30:00
      log_every_n_steps: 1
      val_check_interval: 200
      limit_val_batches: 50
      limit_test_batches: 50
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
    exp_manager:
      explicit_log_dir: /workspace/train/results
      exp_dir: null
      name: megatron_gpt
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: nemo_gpt3
        name: gpt3_5b
      resume_if_exists: true
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: val_loss
        save_top_k: 10
        mode: min
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_gpt--{val_loss:.2f}-{step}-{consumed_samples}
        model_parallel_size: 2
      log_step_timing: true
      step_timing_kwargs:
        sync_cuda: true
        buffer_size: 5
    model:
      micro_batch_size: 8
      global_batch_size: 2048
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      resume_from_checkpoint: null
      encoder_seq_length: 2048
      max_position_embeddings: 2048
      num_layers: 24
      hidden_size: 4096
      ffn_hidden_size: 16384
      num_attention_heads: 32
      init_method_std: 0.01
      hidden_dropout: 0.1
      kv_channels: null
      apply_query_key_layer_scaling: true
      layernorm_epsilon: 1.0e-05
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      persist_layer_norm: true
      gradient_as_bucket_view: true
      grad_div_ar_fusion: true
      gradient_accumulation_fusion: true
      bias_activation_fusion: true
      bias_dropout_add_fusion: true
      masked_softmax_fusion: true
      activations_checkpoint_granularity: selective
      activations_checkpoint_method: block
      activations_checkpoint_num_layers: 0
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      tokenizer:
        library: megatron
        type: GPT2BPETokenizer
        model: null
        delimiter: null
        vocab_file: /workspace/data/bpe/vocab.json
        merge_file: /workspace/data/bpe/merges.txt
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: true
      grad_allreduce_chunk_size_mb: 125
      transformer_engine: true
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: true
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1024
      fp8_amax_compute_algo: max
      use_emha: false
      ub_tp_comm_overlap: false
      seed: 1234
      sync_batch_comm: false
      use_cpu_initialization: false
      onnx_safe: false
      apex_transformer_log_level: 30
      nsys_profile:
        enabled: false
        trace:
        - nvtx
        - cuda
        start_step: 10
        end_step: 10
        ranks:
        - 0
        gen_shape: false
      optim:
        name: distributed_fused_adam
        bucket_cap_mb: 200
        overlap_grad_sync: true
        overlap_param_sync: true
        contiguous_grad_buffer: true
        lr: 0.00016
        weight_decay: 0.1
        betas:
        - 0.9
        - 0.95
        sched:
          name: CosineAnnealing
          warmup_steps: 115
          constant_steps: 12500
          min_lr: 1.6e-05
      data:
        data_impl: mmap
        splits_string: 99990,8,2
        seq_length: 2048
        skip_warmup: true
        num_workers: 2
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        index_mapping_dir: null
        data_prefix:
        - 0.5
        - /workspace/data/my-gpt3_00_text_document
        - 0.5
        - /workspace/data/my-gpt3_01_text_document